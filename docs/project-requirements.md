# LinkedIn Insights Agent MVP Requirements

## Purpose

- Deliver a hosted LinkedIn insights workflow so non-technical users can configure, execute, and download expert analyses without local tooling.
- Reuse the existing CLI scraper/analyzer while layering remote execution, telemetry, and Lovable UI touchpoints that are production ready.
- Capture telemetry needed for pricing experiments (run counts, Anthropic token usage) and surface usage feedback loops.

## Target Users and Jobs

- **Solo operators / early-stage founders** – validate LinkedIn messaging and identify high-performing content to emulate.
- **Content strategists** – collect competitive intelligence across multiple experts and track action plans generated by the analyzer.
- **Internal GTM team** – monitor run volumes, enforce free-tier limits, and review telemetry for scaling decisions.

## Key Scenarios

- **Configure and queue a run**: Authenticated Lovable user selects experts, post limits, and optional filters; submits to API; run stored and queued.
- **Handle login and session reuse**: Worker detects missing LinkedIn cookies, opens Browserless session, asks user to authenticate, and stores encrypted cookies for future runs.
- **Generate insights and artifacts**: Worker scrapes posts, invokes analyzer, produces consolidated instructions plus per-expert Markdown, uploads to Supabase storage, and updates run status.
- **Monitor usage and limits**: API enforces five-run free allowance per user, exposes usage endpoints; UI surfaces remaining runs and token cost estimates.

## Functional Requirements

- **Authentication & Authorization**
  - API endpoints accept Supabase JWT bearer tokens; verify signature and expiry.
  - Successful auth ensures user upsert into Supabase `profiles` table (or equivalent) and ties run ownership to `userId`.
- **Run Submission & Management**
  - Endpoint `POST /runs` accepts run config (experts, per-expert post cap, analysis options) and validates against quotas.
  - Enforce monthly allowance: default five completed runs; configurable via environment overrides.
  - Provide `GET /runs/:id` and `GET /runs` for status polling; include artifact download URLs when available.
- **Queue & Worker Behavior**
  - API enqueues BullMQ job with run metadata and user context.
  - Worker requeues runs needing LinkedIn login with `needs_login` status while keeping queue depth manageable.
  - Run timeout budget is 20 minutes; worker emits structured logs for state transitions and failures.
- **Scraping & Analysis**
  - Reuse existing scraping/analyzer logic by wrapping it with remote session handling and Supabase storage uploads.
  - Support up to 10 experts per run; default to 200 posts/expert; allow user overrides below the cap.
  - Analyzer output includes consolidated instructions and per-expert knowledge bundles saved as Markdown artifacts.
- **Telemetry & Usage Tracking**
  - Record Anthropic token consumption per run for future pricing.
  - Persist run events (queued, needs_login, processing, completed, failed) and queue depth snapshots for observability.
  - Surface metrics via Supabase tables accessible to Lovable dashboards.
- **Frontend Contract**
  - Lovable UI consumes `/runs` and `/usage` endpoints, displays login prompts, status updates, remaining run counts, and download links.
  - UI streams Browserless session when login is required and handles requeue messaging.
- **Storage & Artifacts**
  - Store artifacts in Supabase Storage bucket `runs` with private access; generate signed URLs for downloads.
  - Encrypt stored LinkedIn cookies using `SESSION_ENCRYPTION_KEY` (AES-256-GCM).

## Non-Functional Requirements

- **Reliability**: Automatic retries for transient scraping failures; queue backlog monitoring via telemetry.
- **Security**: No storage of raw LinkedIn credentials; only encrypted session cookies; secrets fetched from hosting environment stores.
- **Performance**: Runs complete within 20 minutes; queue throughput dimensioned for ~500 Browserless sessions/month.
- **Scalability**: Architecture supports scaling API and worker independently; configuration for Redis URL and Browserless endpoints is environment-driven.

## Out of Scope (MVP)

- Paid tiers, seat management, and billing flows.
- Automated LinkedIn authentication or headless credential storage.
- Insight explorer, scheduling, alerts, and playbook builder features.
- Third-party integrations (Zapier, REST API) and landing page changes.

## Data & Persistence

- Supabase houses user profiles, run metadata, telemetry tables (`runs`, `usage_counters`), and artifact metadata.
- Upstash Redis (or compatible) backs BullMQ queue; credentials stored in environment variables.
- Browserless provides interactive Puppeteer sessions; cookies persisted encrypted for reuse.

## Success Metrics

- MVP launch supporting at least 20 pilot users with ≤5% failed runs due to system errors.
- Accurate enforcement of free-tier run allowance and telemetry capture for 100% of completed runs.
- Median end-to-end run duration under 15 minutes.

## Risks & Mitigations

- **Browserless login friction** – Provide clear UI prompts and requeue logic; log needs-login events for support.
- **Queue backlog** – Emit queue depth metrics; alert when waiting jobs exceed thresholds.
- **Analyzer regressions** – Add unit coverage for per-expert bundles; monitor token costs for anomalies.
- **Supabase schema drift** – Manage via versioned migrations and rollback scripts; document in cutover checklist.

## Acceptance Criteria

- Documentation artifacts (`project-requirements`, `telemetry`, `frontend-contract`, `cutover`) reviewed and approved.
- Lovable UI triggers full run, handles login flow, and receives downloadable artifact links.
- Telemetry dashboards (usage counts, token totals) reflect latest completed runs with accurate status transitions.

## Open Questions

- Final hosting choices for API and worker (Render vs Fly.io) and associated deployment pipeline.
- Desired retention window for artifacts and session cookies; current assumption is indefinite until manual purge.
- Pricing strategy beyond free tier once telemetry validates token consumption patterns.
