Plan ID: 20250214-1735-lyn  
Owner: codex  
Branch: feature/web-mvp-platform  
Target services or packages: Lovable UI, /docs, /supabase, /services/api, /services/worker, /src  
Estimated effort: L  
Risk rating: Medium – Coordinating API, worker, Browserless sessions, and Supabase introduces multiple integration points that must align precisely.

## Objectives
- Deliver a hosted MVP that lets individual users configure and execute LinkedIn insight scrapes without local tooling.  
- Enforce a free-tier allowance (five runs) while collecting run-level token telemetry for future pricing analysis.  
- Reuse existing scraping/analyzer logic where reasonable, adding remote Browserless support and Supabase persistence.  
- Produce implementation-ready documentation (requirements, telemetry, cutover) so agents can execute confidently.

## Scope – In
- Requirements documentation for MVP personas, flows, and success metrics.  
- Supabase schema, helper function, and rollback script.  
- Express API service that verifies Supabase JWTs, persists data in Supabase, enqueues BullMQ jobs, and exposes run/usage endpoints.  
- Worker service (BullMQ consumer) handling Browserless login prompts, cookie encryption, scraping, analysis, artifact generation, and telemetry updates.  
- Worker-specific remote scraper abstraction (no changes to existing CLI scraper).  
- Telemetry/logging guidelines, frontend contract, and cutover/QA checklist docs.

## Scope – Out
- Priority-two features (insight explorer, scheduling, alerts, playbook builder).  
- Team workspaces, seat management, enterprise compliance tooling, billing flows.  
- External integrations (Zapier, REST API) and landing-page updates.  
- Automated LinkedIn auth without user interaction; login remains manual in a streamed browser window.

## Assumptions and Constraints
- Supabase Auth issues JWT access tokens; the Lovable front end (or any client) forwards `Authorization: Bearer <token>` to the Express API, which verifies it using the Supabase JWT secret.  
- A standalone Supabase project (URL, service role key, storage bucket) and Redis/Upstash instance are provisioned before implementation.  
- Browserless (or equivalent) supports interactive sessions (connect URL + WS endpoint) and allows cookie export post-login.  
- Runs must complete within ~20 minutes; per-run cap: max 10 experts, default 200 posts/expert (user may lower).  
- Anthropic API usage remains within existing analyzer module; token counts are returned for telemetry.  
- Network egress to LinkedIn is rate-limited; scraper must tolerate occasional MFA prompts or session expiry.

## Architecture and Data Flow Summary
1. Lovable front end authenticates via Supabase and calls API endpoints with a Supabase JWT bearer token.  
2. API verifies the token, upserts user in Supabase, enforces monthly run limit, stores run config, and enqueues a BullMQ job.  
3. Worker dequeues jobs, checks for active encrypted LinkedIn session cookies; if absent it provisions a Browserless session, marks run `needs_login`, and requeues.  
4. User logs into LinkedIn via streamed Browserless UI. Worker reconnects, captures cookies, encrypts & stores them, then launches a remote scraper using those cookies.  
5. Remote scraper gathers posts per expert; analyzer produces insights/templates; formatter emits consolidated instructions + per-expert knowledge base Markdown. Artifacts upload to Supabase storage.  
6. Worker updates telemetry (tokens, cost estimate), marks run complete, and API exposes status/usage endpoints. Lovable UI polls and presents downloads.

## Environment Prerequisites & External Dependencies
- **Supabase project**: create a new project, capture `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`, `SUPABASE_ANON_KEY`, and `SUPABASE_JWT_SECRET` (Settings → API). Enable storage and create a private bucket named `runs`.  
- **Supabase Auth**: configure email link auth (or desired providers) so the front end can obtain JWTs; ensure JWT expiry is suitable for API calls.  
- **Redis queue**: provision an Upstash (or similar) Redis instance, record the connection URL for `RUN_QUEUE_REDIS_URL`.  
- **Browserless (or equivalent)**: open an account capable of ~500 interactive sessions/month with per-minute pricing in cents; gather `BROWSERLESS_HTTP_URL`, `BROWSERLESS_WS_URL`, and `BROWSERLESS_TOKEN`.  
- **Secrets management**: decide where API/worker environment variables live (e.g., Render, Fly.io, Vercel). Required variables:  
  - `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`, `SUPABASE_JWT_SECRET`, `SUPABASE_ANON_KEY`  
  - `RUN_QUEUE_REDIS_URL`  
  - `BROWSERLESS_HTTP_URL`, `BROWSERLESS_WS_URL`, `BROWSERLESS_TOKEN`  
  - `SESSION_ENCRYPTION_KEY` (32+ char random string)  
  - `COST_PER_M_TOKEN`, `RUN_LIMIT`, `RUN_ARTIFACT_BUCKET` (optional overrides)  
- **Local tooling**: Supabase CLI for applying migrations; Node.js ≥18 for running API/worker services locally if needed.

## Work Plan (Tasks)
- **T-001** – Author `docs/project-requirements.md`.  
- **T-002** – Create Supabase migration + rollback for MVP schema.  
- **T-003** – Scaffold Express API (Supabase JWT auth, Supabase persistence, BullMQ enqueue, usage limits, unit test).  
- **T-004** – Build worker service (config, login helpers, remote scraper, run processor, test).  
- **T-005** – Document telemetry & logging conventions.  
- **T-006** – Document frontend integration contract.  
- **T-007** – Document cutover checklist and manual QA steps.

## File Operations Table
| Path | Op | Touch type | Owner | Risk |
| --- | --- | --- | --- | --- |
| docs/project-requirements.md | create | doc | product-eng | Low |
| supabase/migrations/0001_mvp_schema.sql | create | config | platform | Medium |
| supabase/migrations/0001_mvp_schema.rollback.sql | create | config | platform | Medium |
| services/api/package.json | create | config | platform | Medium |
| services/api/tsconfig.json | create | config | platform | Low |
| services/api/src/config.ts | create | code | platform | Medium |
| services/api/src/logger.ts | create | code | platform | Low |
| services/api/src/supabase.ts | create | code | platform | Medium |
| services/api/src/types.ts | create | code | platform | Medium |
| services/api/src/queue.ts | create | code | platform | Medium |
| services/api/src/middleware/auth.ts | create | code | platform | Medium |
| services/api/src/services/run-service.ts | create | code | platform | High |
| services/api/src/routes/runs.ts | create | code | platform | High |
| services/api/src/routes/usage.ts | create | code | platform | Medium |
| services/api/src/index.ts | create | code | platform | High |
| services/api/tests/usage-limit.test.ts | create | code | qa | Medium |
| services/worker/package.json | create | config | platform | High |
| services/worker/tsconfig.json | create | config | platform | Low |
| services/worker/src/config.ts | create | code | platform | Medium |
| services/worker/src/logger.ts | create | code | platform | Low |
| services/worker/src/supabase.ts | create | code | platform | Medium |
| services/worker/src/browserless-client.ts | create | code | platform | High |
| services/worker/src/browser-login.ts | create | code | platform | High |
| services/worker/src/queue.ts | create | code | platform | Medium |
| services/worker/src/storage.ts | create | code | platform | Medium |
| services/worker/src/remote-scraper.ts | create | code | platform | High |
| services/worker/src/knowledge-bundler.ts | create | code | platform | Medium |
| services/worker/src/run-processor.ts | create | code | platform | High |
| services/worker/src/index.ts | create | code | platform | High |
| services/worker/tests/run-processor.test.ts | create | code | qa | Medium |
| docs/telemetry-and-logging.md | create | doc | platform | Low |
| docs/frontend-contract.md | create | doc | product-eng | Low |
| docs/cutover-checklist.md | create | doc | product-eng | Low |

## Implementation Details per Task

### T-001 – MVP Requirements Doc
- **Summary**: Publish canonical MVP requirements (personas, flows, success metrics).  
- **Rationale**: Aligns all agents on scope and informs later design decisions.  
- **Inputs**: Stakeholder decisions captured in briefing.  
- **Outputs**: `docs/project-requirements.md`.  
- **File Ops**: create `docs/project-requirements.md`.

```markdown
# LinkedIn Insights Agent – MVP Requirements

## 1. Vision & Goals
- Launch a self-service web experience for individual operators to generate AI-ready insight packs from LinkedIn experts.  
- Optimize for rapid delivery while collecting telemetry needed for future pricing.  
- Maintain compatibility with existing scraping/analyzer logic while exposing it via web workflows.

## 2. Personas
- Solo sales/marketing operator validating messaging.  
- Consultant producing playbooks for clients.  
- AI prompt engineer building domain-specific knowledge bases.

## 3. Journey Overview
1. Register / log in (Lovable UI using Supabase Auth).  
2. View a pre-generated sample output bundle (hosted in Supabase storage) without connecting LinkedIn.  
3. Configure a run by pasting LinkedIn profile URLs, specifying focus topics, and selecting output format (`AI Project Files`, `Written Brief`).  
4. Trigger run → if session missing, user logs into LinkedIn through streamed Browserless browser.  
5. Monitor run status, then open the completed run to download the consolidated instructions file and per-expert knowledge base files.  
6. Review usage dashboard (runs remaining out of 5).

## 4. Functional Requirements
- Lovable UI delegates authentication to Supabase Auth (email + magic link).  
- Backend stores Supabase `users` row keyed by `auth_user_id` from the JWT.  
- Display one-time “manual LinkedIn login” notice.

### 4.2 Run Configuration
- Input: array of LinkedIn profile URLs (validate format, dedupe); enforce max 10 profiles per run.  
- Topics: free-form comma-separated list with suggested chips  
  (`sales`, `prospecting`, `outreach`, `fundraising`, `marketing`, `branding`, `leadership`, `negotiation`, `writing`, `productivity`, `personal_branding`, `hiring`, `investing`, `design`, `engineering`, `ai`, `strategy`, `retail`, `ecommerce`, `advertising`, `pricing`, `communication`, `public_speaking`, `psychology`).  
- Output format toggle: `AI Project Files` or `Written Brief`.  
- Optional run nickname.  
- Optional per-run post limit (default 200 per expert, user may set lower value to manage runtime).

### 4.3 Run Execution
- Limit: 5 successful runs per user until billing is introduced.  
- Queue job with Supabase metadata; worker consumes via external Redis (BullMQ).  
- Worker fetches/stores artifacts in Supabase storage bucket `runs`.  
- Mark run states: `queued`, `needs_login`, `running`, `failed`, `completed`.  
- Manual login flow: worker provisions Browserless session; API relays login URL to client; upon success cookies encrypted and reused.

### 4.4 Results
- One consolidated instructions Markdown file describing how downstream AI projects should apply the collected knowledge.  
- Individual expert knowledge base Markdown files (one per profile) containing scraped tactics, strategies, templates, and metadata.  
- Optional structured JSON retained server-side for future integrations (not exposed in MVP UI).

### 4.5 Usage & Telemetry
- Usage counter table keyed by month to track `runs_used` and `tokens_used` for each user (enforces 5-run allowance).  
- Capture Anthropic token counts and elapsed duration per run.  
- Store per-run cost estimates on `runs.cost_estimate`; aggregate for dashboards later.

## 5. Non-Functional Requirements
- Encryption at rest for LinkedIn cookies (AES-256-GCM with per-user key derived from server secret).  
- Hard 20-minute job timeout with retries (max 1 retry).  
- Enforce max 10 experts per run and default 200-post scrape limit (user may opt down) to stay within timeout budget.  
- Observability: structured logs (pino), Supabase row for run events.  
- Compliance notice: “You acknowledge you are using this tool at your own risk and automated access to LinkedIn may violate their terms.”

## 6. Open Questions / Follow-ups
- Select Browserless/Playwright managed provider that can handle ~500 interactive logins/month with per-minute pricing measured in cents and meets any data residency/security constraints.  
- Signed download URLs/webhooks decision: default to API-generated signed URLs for MVP (simpler deployment, shared auth logic); reassess Supabase Edge Functions if latency, vendor lock-in, or multi-region needs arise.

## 7. Success Metrics
- 5 pilot users complete at least one run in first week.  
- <5% run failure rate excluding LinkedIn login refusal.  
- All runs produce consolidated + per-expert Markdown artifacts.  
- Usage dashboard accurately decrements remaining runs.

## 8. Future Enhancements (Not in MVP Scope)
- Insight explorer with filters.  
- Saved configurations and schedules.  
- Team workspaces, extra seats.  
- API integrations (Zapier, REST).  
- Advanced compliance (dedicated proxies, SOC2 reporting).
```

- **Tests**: none.  
- **Telemetry**: none.  
- **Rollback**: delete the doc if scope changes.

---

### T-002 – Supabase Schema & Rollback
- **Summary**: Create initial Supabase schema (users, sessions, runs, events, artifacts, usage counters) plus helper function and rollback.  
- **Rationale**: Establishes persistent storage and atomic usage tracking for API/worker.  
- **Inputs**: Requirements doc; Supabase connection details.  
- **Outputs**: `0001_mvp_schema.sql` and rollback.  
- **File Ops**: create both migration files.

```sql
-- supabase/migrations/0001_mvp_schema.sql
BEGIN;

CREATE EXTENSION IF NOT EXISTS "pgcrypto";
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE OR REPLACE FUNCTION public.set_updated_at()
RETURNS trigger
LANGUAGE plpgsql
AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$;

CREATE TABLE public.users (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  auth_user_id uuid NOT NULL UNIQUE,
  email text NOT NULL,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  updated_at timestamptz NOT NULL DEFAULT NOW()
);
CREATE TRIGGER trg_users_updated_at
BEFORE UPDATE ON public.users
FOR EACH ROW EXECUTE PROCEDURE public.set_updated_at();

CREATE TABLE public.linked_sessions (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id uuid NOT NULL REFERENCES public.users(id) ON DELETE CASCADE,
  provider text NOT NULL DEFAULT 'browserless',
  encrypted_payload text NOT NULL,
  encryption_algorithm text NOT NULL DEFAULT 'aes-256-gcm',
  expires_at timestamptz,
  is_active boolean NOT NULL DEFAULT true,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  updated_at timestamptz NOT NULL DEFAULT NOW()
);
CREATE TRIGGER trg_linked_sessions_updated_at
BEFORE UPDATE ON public.linked_sessions
FOR EACH ROW EXECUTE PROCEDURE public.set_updated_at();
CREATE INDEX idx_linked_sessions_user_active ON public.linked_sessions(user_id, is_active);

CREATE TYPE public.run_status AS ENUM ('queued','needs_login','running','failed','completed');

CREATE TABLE public.runs (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id uuid NOT NULL REFERENCES public.users(id) ON DELETE CASCADE,
  status public.run_status NOT NULL DEFAULT 'queued',
  config jsonb NOT NULL,
  run_nickname text,
  token_estimate bigint DEFAULT 0,
  cost_estimate numeric(10,4),
  needs_login_url text,
  started_at timestamptz,
  completed_at timestamptz,
  failure_reason text,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  updated_at timestamptz NOT NULL DEFAULT NOW()
);
CREATE TRIGGER trg_runs_updated_at
BEFORE UPDATE ON public.runs
FOR EACH ROW EXECUTE PROCEDURE public.set_updated_at();
CREATE INDEX idx_runs_user_created_at ON public.runs(user_id, created_at DESC);

CREATE TABLE public.run_events (
  id bigserial PRIMARY KEY,
  run_id uuid NOT NULL REFERENCES public.runs(id) ON DELETE CASCADE,
  event_type text NOT NULL,
  payload jsonb,
  created_at timestamptz NOT NULL DEFAULT NOW()
);
CREATE INDEX idx_run_events_run_time ON public.run_events(run_id, created_at);

CREATE TABLE public.run_artifacts (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  run_id uuid NOT NULL REFERENCES public.runs(id) ON DELETE CASCADE,
  artifact_type text NOT NULL,
  storage_path text NOT NULL,
  content_sha256 text NOT NULL,
  bytes bigint NOT NULL,
  created_at timestamptz NOT NULL DEFAULT NOW()
);

CREATE TABLE public.usage_counters (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id uuid NOT NULL REFERENCES public.users(id) ON DELETE CASCADE,
  period_start date NOT NULL,
  runs_used integer NOT NULL DEFAULT 0,
  tokens_used bigint NOT NULL DEFAULT 0,
  UNIQUE (user_id, period_start)
);

CREATE OR REPLACE FUNCTION public.increment_run_usage(p_user_id uuid, p_period_start date)
RETURNS void
LANGUAGE plpgsql
AS $$
BEGIN
  INSERT INTO public.usage_counters (user_id, period_start, runs_used, tokens_used)
  VALUES (p_user_id, p_period_start, 1, 0)
  ON CONFLICT (user_id, period_start)
  DO UPDATE SET runs_used = public.usage_counters.runs_used + 1;
END;
$$;

COMMIT;
```

```sql
-- supabase/migrations/0001_mvp_schema.rollback.sql
BEGIN;
DROP FUNCTION IF EXISTS public.increment_run_usage(uuid, date);
DROP TABLE IF EXISTS public.run_artifacts CASCADE;
DROP TABLE IF EXISTS public.run_events CASCADE;
DROP TABLE IF EXISTS public.runs CASCADE;
DROP TYPE IF EXISTS public.run_status;
DROP TABLE IF EXISTS public.linked_sessions CASCADE;
DROP TABLE IF EXISTS public.usage_counters CASCADE;
DROP TABLE IF EXISTS public.users CASCADE;
DROP FUNCTION IF EXISTS public.set_updated_at();
COMMIT;
```

- **Tests**: apply migrations via Supabase CLI (future).  
- **Telemetry**: none.  
- **Rollback**: apply rollback script.

---

- **Summary**: Build Express API that verifies Supabase JWTs, integrates with Supabase data layer, enqueues BullMQ jobs, exposes run/usage endpoints, and includes unit tests.  
- **Rationale**: Provides secure REST interface for the Lovable UI while enforcing run limits and recording telemetry.  
- **Inputs**: Requirements doc, migration schema, Supabase service role key + JWT secret, Redis credentials.  
- **Outputs**: API service directory with config, routes, service layer, and test.  
- **File Ops**: create the listed API files.

```json
{
  "name": "linkedin-insights-api",
  "version": "0.1.0",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "tsx watch src/index.ts",
    "start": "node --loader ts-node/esm src/index.ts",
    "build": "tsc -p tsconfig.json",
    "test": "vitest"
  },
  "dependencies": {
    "@supabase/supabase-js": "^2.45.3",
    "bullmq": "^4.17.0",
    "cors": "^2.8.5",
    "dotenv": "^16.4.7",
    "express": "^4.19.2",
    "jsonwebtoken": "^9.0.2",
    "pino": "^9.3.2",
    "uuid": "^9.0.1",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@types/express": "^4.17.21",
    "@types/jsonwebtoken": "^9.0.6",
    "@types/node": "^20.14.9",
    "supertest": "^6.3.4",
    "ts-node": "^10.9.2",
    "tsx": "^4.20.5",
    "typescript": "^5.9.2",
    "vitest": "^1.6.1"
  }
}
```

```json
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "rootDir": "./src",
    "module": "ESNext",
    "moduleResolution": "node",
    "target": "ES2020",
    "resolveJsonModule": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["src/**/*.ts", "tests/**/*.ts"]
}
```

```typescript
// services/api/src/config.ts
import dotenv from 'dotenv';

dotenv.config();

function requireEnv(name: string): string {
  const value = process.env[name];
  if (!value) {
    throw new Error(`Missing required env var ${name}`);
  }
  return value;
}

export const config = {
  port: Number(process.env.API_PORT ?? 4000),
  supabaseUrl: requireEnv('SUPABASE_URL'),
  supabaseServiceKey: requireEnv('SUPABASE_SERVICE_ROLE_KEY'),
  queueRedisUrl: requireEnv('RUN_QUEUE_REDIS_URL'),
  supabaseJwtSecret: requireEnv('SUPABASE_JWT_SECRET'),
  runLimit: Number(process.env.RUN_LIMIT ?? 5),
  costPerMillionTokens: Number(process.env.COST_PER_M_TOKEN ?? 15),
  corsOrigin: process.env.API_CORS_ORIGIN ?? '*'
} as const;
```

```typescript
// services/api/src/logger.ts
import pino from 'pino';

export const logger = pino({
  level: process.env.LOG_LEVEL ?? 'info',
  transport: process.env.NODE_ENV === 'production' ? undefined : {
    target: 'pino-pretty',
    options: { colorize: true }
  }
});
```

```typescript
// services/api/src/supabase.ts
import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { config } from './config.js';

let client: SupabaseClient | null = null;

export function getSupabase(): SupabaseClient {
  if (!client) {
    client = createClient(config.supabaseUrl, config.supabaseServiceKey, {
      auth: { persistSession: false },
      global: { headers: { 'X-Client-Info': 'linkedin-insights-api' } }
    });
  }
  return client;
}
```

```typescript
// services/api/src/types.ts
export interface AuthContext {
  userId: string;
  email: string;
  role?: string;
}

export interface RunConfigInput {
  profileUrls: string[];
  topics: string[];
  outputFormat: 'ai-ready' | 'briefing';
  nickname?: string;
  postLimit?: number;
}
```

```typescript
// services/api/src/queue.ts
import { Queue } from 'bullmq';
import { config } from './config.js';

export const runQueue = new Queue('insight-runs', {
  connection: { url: config.queueRedisUrl },
  defaultJobOptions: {
    attempts: 3,
    backoff: { type: 'exponential', delay: 2000 },
    removeOnComplete: true,
    removeOnFail: 500
  }
});
```

```typescript
// services/api/src/middleware/auth.ts
import { Request, Response, NextFunction } from 'express';
import jwt from 'jsonwebtoken';
import { config } from '../config.js';
import { AuthContext } from '../types.js';

declare global {
  namespace Express {
    interface Request {
      auth?: AuthContext;
    }
  }
}

export function supabaseAuth(req: Request, res: Response, next: NextFunction) {
  const header = req.header('authorization');
  const token = header?.startsWith('Bearer ') ? header.slice(7) : null;
  if (!token) {
    return res.status(401).json({ error: 'Missing bearer token' });
  }

  try {
    const decoded = jwt.verify(token, config.supabaseJwtSecret) as { sub: string; email?: string; role?: string };
    if (!decoded?.sub) {
      return res.status(401).json({ error: 'Invalid token payload' });
    }
    req.auth = {
      userId: decoded.sub,
      email: decoded.email ?? '',
      role: decoded.role
    };
    next();
  } catch (error) {
    return res.status(401).json({ error: 'Invalid token' });
  }
}
```

```typescript
// services/api/src/services/run-service.ts
import { z } from 'zod';
import { getSupabase } from '../supabase.js';
import { config } from '../config.js';
import { runQueue } from '../queue.js';
import { AuthContext, RunConfigInput } from '../types.js';
import { logger } from '../logger.js';

const schema = z.object({
  profileUrls: z.array(z.string().url()).nonempty(),
  topics: z.array(z.string().min(1)).max(24),
  outputFormat: z.enum(['ai-ready', 'briefing']),
  nickname: z.string().max(80).optional(),
  postLimit: z.number().int().min(10).max(200).optional()
});

function canonicalizeProfile(url: string): string {
  return url.trim().replace(/(\?.*)$/, '').replace(/\/$/, '');
}

function currentPeriodStart(): string {
  const now = new Date();
  return `${now.getUTCFullYear()}-${String(now.getUTCMonth() + 1).padStart(2, '0')}-01`;
}

export async function ensureUserRecord(auth: AuthContext): Promise<string> {
  const supabase = getSupabase();
  const { data, error } = await supabase
    .from('users')
    .upsert({
      auth_user_id: auth.userId,
      email: auth.email
    }, { onConflict: 'auth_user_id' })
    .select('id')
    .single();

  if (error) {
    logger.error({ error }, 'Failed to upsert user');
    throw error;
  }
  return data.id;
}

export async function validateRunConfig(input: unknown): Promise<RunConfigInput> {
  const parsed = schema.parse(input);
  const uniqueProfiles = [...new Set(parsed.profileUrls.map(canonicalizeProfile))];
  if (uniqueProfiles.length > 10) {
    throw new Error('You can analyze up to 10 experts per run');
  }

  return {
    ...parsed,
    profileUrls: uniqueProfiles,
    topics: parsed.topics.map((topic) => topic.trim()).filter(Boolean),
    postLimit: parsed.postLimit ?? 200
  };
}

export async function checkRunLimit(userId: string): Promise<number> {
  const supabase = getSupabase();
  const periodStart = currentPeriodStart();

  const { data, error } = await supabase
    .from('usage_counters')
    .select('runs_used')
    .eq('user_id', userId)
    .eq('period_start', periodStart)
    .single();

  if (error && error.code !== 'PGRST116') {
    throw error;
  }

  const runsUsed = data?.runs_used ?? 0;
  if (runsUsed >= config.runLimit) {
    throw Object.assign(new Error('Run limit reached'), { remaining: 0 });
  }
  return config.runLimit - runsUsed;
}

export async function incrementRunUsage(userId: string) {
  const supabase = getSupabase();
  const periodStart = currentPeriodStart();

  const { error } = await supabase.rpc('increment_run_usage', {
    p_user_id: userId,
    p_period_start: periodStart
  });

  if (error) {
    throw error;
  }
}

export async function createRun(userId: string, configInput: RunConfigInput) {
  const supabase = getSupabase();
  const { data, error } = await supabase.from('runs')
    .insert({
      user_id: userId,
      config: configInput,
      run_nickname: configInput.nickname ?? null
    })
    .select('id, status, created_at')
    .single();

  if (error) {
    throw error;
  }

  await runQueue.add('run', { runId: data.id, userId });
  return data;
}

export async function listRuns(userId: string) {
  const supabase = getSupabase();
  const { data, error } = await supabase.from('runs')
    .select('id, status, run_nickname, created_at, completed_at, failure_reason, token_estimate, cost_estimate')
    .eq('user_id', userId)
    .order('created_at', { ascending: false })
    .limit(100);

  if (error) {
    throw error;
  }
  return data;
}

export async function getRun(userId: string, runId: string) {
  const supabase = getSupabase();
  const { data, error } = await supabase.from('runs')
    .select('id, status, config, run_nickname, created_at, completed_at, failure_reason, token_estimate, cost_estimate, needs_login_url')
    .eq('user_id', userId)
    .eq('id', runId)
    .single();

  if (error) {
    throw error;
  }
  return data;
}
```

```typescript
// services/api/src/routes/runs.ts
import { Router } from 'express';
import { supabaseAuth } from '../middleware/auth.js';
import {
  ensureUserRecord,
  validateRunConfig,
  checkRunLimit,
  incrementRunUsage,
  createRun,
  listRuns,
  getRun
} from '../services/run-service.js';
import { logger } from '../logger.js';

export const runsRouter = Router();

runsRouter.use(supabaseAuth);

runsRouter.post('/', async (req, res) => {
  try {
    const auth = req.auth!;
    const userId = await ensureUserRecord(auth);
    await checkRunLimit(userId);
    const config = await validateRunConfig(req.body);
    const run = await createRun(userId, config);
    await incrementRunUsage(userId);
    res.status(201).json({ run });
  } catch (error: any) {
    logger.error({ error }, 'Failed to create run');
    if (error.message === 'Run limit reached') {
      res.status(429).json({ error: error.message });
    } else if (error.message === 'You can analyze up to 10 experts per run') {
      res.status(400).json({ error: error.message });
    } else if (error?.issues) {
      res.status(400).json({ error: 'Invalid run configuration', details: error.issues });
    } else {
      res.status(500).json({ error: 'Unexpected error' });
    }
  }
});

runsRouter.get('/', async (req, res) => {
  try {
    const userId = await ensureUserRecord(req.auth!);
    const runs = await listRuns(userId);
    res.json({ runs });
  } catch (error) {
    logger.error({ error }, 'Failed to list runs');
    res.status(500).json({ error: 'Unexpected error' });
  }
});

runsRouter.get('/:runId', async (req, res) => {
  try {
    const userId = await ensureUserRecord(req.auth!);
    const run = await getRun(userId, req.params.runId);
    res.json({ run });
  } catch (error: any) {
    logger.error({ error }, 'Failed to fetch run');
    if (error?.code === 'PGRST116') {
      res.status(404).json({ error: 'Run not found' });
    } else {
      res.status(500).json({ error: 'Unexpected error' });
    }
  }
});
```

```typescript
// services/api/src/routes/usage.ts
import { Router } from 'express';
import { supabaseAuth } from '../middleware/auth.js';
import { ensureUserRecord } from '../services/run-service.js';
import { getSupabase } from '../supabase.js';
import { config } from '../config.js';

export const usageRouter = Router();

usageRouter.use(supabaseAuth);

function currentPeriodStart(): string {
  const now = new Date();
  return `${now.getUTCFullYear()}-${String(now.getUTCMonth() + 1).padStart(2, '0')}-01`;
}

usageRouter.get('/', async (req, res) => {
  const userId = await ensureUserRecord(req.auth!);
  const supabase = getSupabase();
  const { data, error } = await supabase
    .from('usage_counters')
    .select('runs_used, tokens_used')
    .eq('user_id', userId)
    .eq('period_start', currentPeriodStart())
    .single();

  if (error && error.code !== 'PGRST116') {
    return res.status(500).json({ error: 'Failed to fetch usage' });
  }

  const runsUsed = data?.runs_used ?? 0;
  const tokensUsed = data?.tokens_used ?? 0;

  res.json({
    runsUsed,
    runsRemaining: Math.max(config.runLimit - runsUsed, 0),
    tokensUsed,
    tokenCostEstimate: Number(((tokensUsed / 1_000_000) * config.costPerMillionTokens).toFixed(4))
  });
});
```

```typescript
// services/api/src/index.ts
import express from 'express';
import cors from 'cors';
import { config } from './config.js';
import { runsRouter } from './routes/runs.js';
import { usageRouter } from './routes/usage.js';
import { logger } from './logger.js';

const app = express();
app.use(cors({ origin: config.corsOrigin, credentials: true }));
app.use(express.json());

app.get('/health', (_req, res) => res.json({ ok: true }));

app.use('/runs', runsRouter);
app.use('/usage', usageRouter);

app.listen(config.port, () => {
  logger.info({ port: config.port }, 'API server listening');
});
```

```typescript
// services/api/tests/usage-limit.test.ts
import { describe, it, expect, beforeEach, vi } from 'vitest';
import * as runService from '../src/services/run-service.js';
import { getSupabase } from '../src/supabase.js';
import { supabaseAuth } from '../src/middleware/auth.js';
import jwt from 'jsonwebtoken';

vi.mock('../src/supabase.js');
vi.mock('jsonwebtoken');

describe('run limit enforcement', () => {
  const mockClient = { from: vi.fn() };

  beforeEach(() => {
    vi.resetAllMocks();
    (getSupabase as unknown as vi.Mock).mockReturnValue(mockClient);
  });

  it('allows runs when under limit', async () => {
    mockClient.from.mockReturnValue({
      select: vi.fn().mockReturnValue({
        eq: vi.fn().mockReturnValue({
          eq: vi.fn().mockReturnValue({
            single: vi.fn().mockResolvedValue({ data: { runs_used: 3 }, error: null })
          })
        })
      })
    });

    const remaining = await runService.checkRunLimit('user-1');
    expect(remaining).toBeGreaterThan(0);
  });

  it('throws when limit reached', async () => {
    mockClient.from.mockReturnValue({
      select: vi.fn().mockReturnValue({
        eq: vi.fn().mockReturnValue({
          eq: vi.fn().mockReturnValue({
            single: vi.fn().mockResolvedValue({ data: { runs_used: 5 }, error: null })
          })
        })
      })
    });

    await expect(runService.checkRunLimit('user-1')).rejects.toThrow('Run limit reached');
  });

  it('rejects missing bearer token', () => {
    const req: any = { header: vi.fn().mockReturnValue(undefined) };
    const res: any = { status: vi.fn().mockReturnValue({ json: vi.fn() }) };
    const next = vi.fn();
    supabaseAuth(req, res, next);
    expect(res.status).toHaveBeenCalledWith(401);
    expect(next).not.toHaveBeenCalled();
  });

  it('accepts valid Supabase JWT', () => {
    const token = 'valid';
    const req: any = { header: vi.fn().mockImplementation((key) => key === 'authorization' ? `Bearer ${token}` : undefined) };
    const res: any = { status: vi.fn().mockReturnValue({ json: vi.fn() }) };
    const next = vi.fn();
    (jwt.verify as unknown as vi.Mock).mockReturnValue({ sub: 'user-id', email: 'a@example.com' });
    supabaseAuth(req, res, next);
    expect(req.auth?.userId).toBe('user-id');
    expect(next).toHaveBeenCalled();
  });
});
```

- **Tests**: `npm test` in `/services/api` (Vitest).  
- **Telemetry**: Provided via pino logger.  
- **Rollback**: remove `/services/api`.

---

### T-004 – Worker Service & Remote Scraper Support
- **Summary**: Build worker service (config, login helpers, BullMQ worker, storage, remote scraper, run processor, unit test) using Browserless sessions and encrypted cookies.  
- **Rationale**: Executes long-running scraping pipeline, manages login handoff, and stores artifacts/telemetry.  
- **Inputs**: Requirements doc, Supabase credentials, Browserless endpoints/token, Redis URL.  
- **Outputs**: Worker service directory with supporting modules and tests.  
- **File Ops**: create listed worker files.

```json
{
  "name": "linkedin-insights-worker",
  "version": "0.1.0",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "tsx watch src/index.ts",
    "start": "node --loader ts-node/esm src/index.ts",
    "build": "tsc -p tsconfig.json",
    "test": "vitest"
  },
  "dependencies": {
    "@supabase/supabase-js": "^2.45.3",
    "bullmq": "^4.17.0",
    "dotenv": "^16.4.7",
    "node-fetch": "^3.3.2",
    "pino": "^9.3.2",
    "puppeteer": "^22.12.0",
    "uuid": "^9.0.1"
  },
  "devDependencies": {
    "@types/node": "^20.14.9",
    "ts-node": "^10.9.2",
    "tsx": "^4.20.5",
    "typescript": "^5.9.2",
    "vitest": "^1.6.1"
  }
}
```

```json
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "rootDir": "./src",
    "module": "ESNext",
    "target": "ES2020",
    "moduleResolution": "node",
    "esModuleInterop": true,
    "skipLibCheck": true
  },
  "include": ["src/**/*.ts", "tests/**/*.ts"]
}
```

```typescript
// services/worker/src/config.ts
import dotenv from 'dotenv';

dotenv.config();

function requireEnv(name: string): string {
  const value = process.env[name];
  if (!value) {
    throw new Error(`Missing env var ${name}`);
  }
  return value;
}

export const config = {
  supabaseUrl: requireEnv('SUPABASE_URL'),
  supabaseServiceKey: requireEnv('SUPABASE_SERVICE_ROLE_KEY'),
  queueRedisUrl: requireEnv('RUN_QUEUE_REDIS_URL'),
  browserlessHttpUrl: requireEnv('BROWSERLESS_HTTP_URL'),
  browserlessWsUrl: requireEnv('BROWSERLESS_WS_URL'),
  browserlessToken: requireEnv('BROWSERLESS_TOKEN'),
  artifactBucket: process.env.RUN_ARTIFACT_BUCKET ?? 'runs',
  maxRunMinutes: Number(process.env.RUN_MAX_MINUTES ?? 20),
  requeueDelayMs: Number(process.env.RUN_REQUEUE_DELAY_MS ?? 5000),
  encryptionKey: requireEnv('SESSION_ENCRYPTION_KEY'),
  costPerMillionTokens: Number(process.env.COST_PER_M_TOKEN ?? 15)
} as const;
```

```typescript
// services/worker/src/logger.ts
import pino from 'pino';

export const logger = pino({
  level: process.env.LOG_LEVEL ?? 'info',
  transport: process.env.NODE_ENV === 'production' ? undefined : {
    target: 'pino-pretty',
    options: { colorize: true }
  }
});
```

```typescript
// services/worker/src/supabase.ts
import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { config } from './config.js';

let client: SupabaseClient | null = null;

export function getSupabase(): SupabaseClient {
  if (!client) {
    client = createClient(config.supabaseUrl, config.supabaseServiceKey, {
      auth: { persistSession: false },
      global: { headers: { 'X-Client-Info': 'linkedin-insights-worker' } }
    });
  }
  return client;
}
```

```typescript
// services/worker/src/browserless-client.ts
import fetch from 'node-fetch';
import puppeteer, { Browser } from 'puppeteer';
import { config } from './config.js';
import { logger } from './logger.js';

export interface BrowserlessSession {
  sessionId: string;
  connectUrl: string;
  wsEndpoint: string;
  expiresAt?: string;
}

export async function createSession(options: { headless: boolean; keepAlive?: boolean }): Promise<BrowserlessSession> {
  const response = await fetch(`${config.browserlessHttpUrl}/sessions?token=${config.browserlessToken}`, {
    method: 'POST',
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({
      headless: options.headless,
      keepAlive: options.keepAlive ?? true,
      blockAds: true,
      recordVideo: false,
      timeout: config.maxRunMinutes * 60 * 1000
    })
  });

  if (!response.ok) {
    logger.error({ status: response.status, text: await response.text() }, 'Failed to create Browserless session');
    throw new Error('Browserless session provision failed');
  }

  const payload = await response.json() as {
    id: string;
    connectUrl: string;
    wsEndpoint: string;
    expiresAt?: string;
  };

  return {
    sessionId: payload.id,
    connectUrl: payload.connectUrl,
    wsEndpoint: payload.wsEndpoint,
    expiresAt: payload.expiresAt
  };
}

export async function connectToSession(wsEndpoint: string): Promise<Browser> {
  const endpoint = `${wsEndpoint}?token=${config.browserlessToken}`;
  return puppeteer.connect({
    browserWSEndpoint: endpoint,
    ignoreHTTPSErrors: true,
    defaultViewport: { width: 1280, height: 720 }
  });
}
```

```typescript
// services/worker/src/browser-login.ts
import crypto from 'crypto';
import { Protocol } from 'puppeteer';
import { createSession, connectToSession, BrowserlessSession } from './browserless-client.js';
import { config } from './config.js';
import { getSupabase } from './supabase.js';
import { logger } from './logger.js';

const ALGORITHM = 'aes-256-gcm';

function encryptionKey(): Buffer {
  return crypto.createHash('sha256').update(config.encryptionKey).digest();
}

export function encryptCookies(cookies: Protocol.Network.Cookie[]): { payload: string } {
  const iv = crypto.randomBytes(12);
  const cipher = crypto.createCipheriv(ALGORITHM, encryptionKey(), iv);
  const serialized = Buffer.from(JSON.stringify(cookies), 'utf-8');
  const cipherText = Buffer.concat([cipher.update(serialized), cipher.final()]);
  const authTag = cipher.getAuthTag();
  const payload = Buffer.concat([iv, authTag, cipherText]).toString('base64');
  return { payload };
}

export function decryptCookies(payload: string): Protocol.Network.Cookie[] {
  const buffer = Buffer.from(payload, 'base64');
  const iv = buffer.subarray(0, 12);
  const authTag = buffer.subarray(12, 28);
  const cipherText = buffer.subarray(28);
  const decipher = crypto.createDecipheriv(ALGORITHM, encryptionKey(), iv);
  decipher.setAuthTag(authTag);
  const decrypted = Buffer.concat([decipher.update(cipherText), decipher.final()]);
  return JSON.parse(decrypted.toString('utf-8'));
}

export async function ensureLoginSession(runId: string, userId: string) {
  const session = await createSession({ headless: false });
  const supabase = getSupabase();

  const { error } = await supabase.from('runs')
    .update({ status: 'needs_login', needs_login_url: session.connectUrl })
    .eq('id', runId);
  if (error) throw error;

  await supabase.from('run_events').insert({
    run_id: runId,
    event_type: 'needs_login',
    payload: {
      sessionId: session.sessionId,
      connectUrl: session.connectUrl,
      wsEndpoint: session.wsEndpoint,
      expiresAt: session.expiresAt
    }
  });

  logger.info({ runId, sessionId: session.sessionId }, 'Login session provisioned');
}

export async function captureCookiesFromSession(session: BrowserlessSession): Promise<Protocol.Network.Cookie[]> {
  const browser = await connectToSession(session.wsEndpoint);
  const pages = await browser.pages();
  const page = pages.length > 0 ? pages[0] : await browser.newPage();

  await page.waitForTimeout(2000);
  const cookies = await page.cookies('https://www.linkedin.com/');
  await browser.disconnect();
  return cookies;
}
```

```typescript
// services/worker/src/queue.ts
import { Worker, Job } from 'bullmq';
import { config } from './config.js';
import { logger } from './logger.js';
import { processRunJob, RunJobData } from './run-processor.js';

export function startWorker() {
  const worker = new Worker<RunJobData>('insight-runs', async (job: Job<RunJobData>) => {
    logger.info({ runId: job.data.runId }, 'Processing run job');
    await processRunJob(job.data, job);
  }, {
    connection: { url: config.queueRedisUrl },
    lockDuration: config.maxRunMinutes * 60 * 1000,
    removeOnComplete: true,
    removeOnFail: 500
  });

  worker.on('completed', (job) => {
    logger.info({ jobId: job.id }, 'Run job completed');
  });
  worker.on('failed', (job, err) => {
    logger.error({ jobId: job?.id, err }, 'Run job failed');
  });

  return worker;
}
```

```typescript
// services/worker/src/storage.ts
import crypto from 'crypto';
import { getSupabase } from './supabase.js';
import { config } from './config.js';

export async function storeArtifact(runId: string, artifactType: string, content: string) {
  const supabase = getSupabase();
  const bytes = Buffer.byteLength(content, 'utf-8');
  const sha = crypto.createHash('sha256').update(content).digest('hex');
  const path = `${runId}/${artifactType}.md`;

  const { error: uploadError } = await supabase.storage.from(config.artifactBucket)
    .upload(path, content, { contentType: 'text/markdown', upsert: true });
  if (uploadError) throw uploadError;

  const { error: insertError } = await supabase.from('run_artifacts').insert({
    run_id: runId,
    artifact_type: artifactType,
    storage_path: path,
    content_sha256: sha,
    bytes
  });
  if (insertError) throw insertError;
}
```

```typescript
// services/worker/src/knowledge-bundler.ts
import { InsightAnalysis, ScrapingConfig } from '../../src/types.js';
import { ThreeFileFormatter } from '../../src/three-file-formatter.js';

export interface KnowledgeBundle {
  instructionsMarkdown: string;
  expertMarkdownFiles: Array<{ username: string; content: string }>;
}

export function buildKnowledgeBundle(
  aggregateAnalysis: InsightAnalysis,
  perExpertAnalysis: Record<string, InsightAnalysis>,
  formatter: ThreeFileFormatter,
  topics: string[],
  defaultPostLimit: number
): KnowledgeBundle {
  const instructionsMarkdown = [
    '# How to Use These Insights',
    '',
    aggregateAnalysis.actionableInstructions && aggregateAnalysis.actionableInstructions.length > 0
      ? aggregateAnalysis.actionableInstructions.map((item) => `- ${item}`).join('\n')
      : '- Prioritize the highest confidence insights and adapt them to your outreach sequences.',
    '',
    'Refer to the individual expert files for detailed tactics and source notes.'
  ].join('\n');

  const expertMarkdownFiles = Object.entries(perExpertAnalysis).map(([username, expertAnalysis]) => {
    const postCount = expertAnalysis.posts?.length ?? defaultPostLimit;
    const knowledgeConfig: ScrapingConfig = {
      linkedinUsername: username,
      focusTopics: topics,
      postLimit: postCount,
      outputFormat: 'instructions'
    };
    const knowledge = formatter.formatThreeFiles(expertAnalysis, knowledgeConfig);
    return {
      username,
      content: [
        `# ${username} Knowledge Base`,
        '',
        knowledge.knowledgeBase
      ].join('\n')
    };
  });

  return { instructionsMarkdown, expertMarkdownFiles };
}
```

```typescript
// services/worker/src/remote-scraper.ts
import puppeteer, { Browser, Page, Protocol } from 'puppeteer';
import * as cheerio from 'cheerio';
import { LinkedInPost } from '../../src/types.js';

interface RemoteScraperOptions {
  wsEndpoint: string;
  token: string;
  cookies: Protocol.Network.Cookie[];
}

interface ScrapeResult {
  allPosts: LinkedInPost[];
  byExpert: Record<string, LinkedInPost[]>;
}

export class RemoteScraper {
  private browser: Browser | null = null;
  private page: Page | null = null;
  private options: RemoteScraperOptions;

  constructor(options: RemoteScraperOptions) {
    this.options = options;
  }

  async init(): Promise<void> {
    if (this.browser) return;
    const endpoint = `${this.options.wsEndpoint}?token=${this.options.token}`;
    this.browser = await puppeteer.connect({
      browserWSEndpoint: endpoint,
      ignoreHTTPSErrors: true,
      defaultViewport: { width: 1280, height: 720 }
    });
    this.page = await this.browser.newPage();
    await this.page.setViewport({ width: 1440, height: 900 });
    await this.applyCookies();
    await this.page.setUserAgent('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36');
  }

  private toCookieParams(cookies: Protocol.Network.Cookie[]): Protocol.Network.CookieParam[] {
    return cookies.map((cookie) => ({
      name: cookie.name,
      value: cookie.value,
      domain: cookie.domain,
      path: cookie.path,
      expires: cookie.expires > 0 ? cookie.expires : undefined,
      httpOnly: cookie.httpOnly,
      secure: cookie.secure,
      sameSite: cookie.sameSite
    }));
  }

  private async applyCookies() {
    if (!this.page) return;
    const params = this.toCookieParams(this.options.cookies);
    if (params.length) {
      await this.page.setCookie(...params);
    }
  }

  async scrapeProfiles(profileUrls: string[], postLimit: number, focusTopics: string[]): Promise<ScrapeResult> {
    if (!this.page) throw new Error('Remote scraper not initialized');
    const allPosts: LinkedInPost[] = [];
    const byExpert: Record<string, LinkedInPost[]> = {};

    for (const profileUrl of profileUrls) {
      const username = profileUrl.replace(/^https:\/\/www\.linkedin\.com\/in\//, '').replace(/\/$/, '');
      const posts = await this.scrapeProfile(username, postLimit);
      byExpert[username] = posts;
      allPosts.push(...posts);
    }

    return { allPosts, byExpert };
  }

  private async ensureLoggedIn(): Promise<void> {
    if (!this.page) return;
    await this.page.goto('https://www.linkedin.com/feed/', { waitUntil: 'domcontentloaded', timeout: 45000 });
    const nav = await this.page.$('.global-nav__primary-link, .scaffold-layout__nav');
    if (!nav) {
      throw new Error('LinkedIn session invalid');
    }
  }

  private async scrapeProfile(username: string, postLimit: number): Promise<LinkedInPost[]> {
    if (!this.page) throw new Error('Remote scraper not initialized');
    await this.ensureLoggedIn();

    const profileUrl = `https://www.linkedin.com/in/${username}/recent-activity/all/`;
    await this.page.goto(profileUrl, { waitUntil: 'domcontentloaded', timeout: 45000 });
    await this.page.waitForTimeout(2000);

    for (let i = 0; i < 6; i++) {
      await this.page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));
      await this.page.waitForTimeout(2000);
    }

    const html = await this.page.content();
    return this.extractPosts(html, username).slice(0, postLimit);
  }

  private extractPosts(html: string, username: string): LinkedInPost[] {
    const $ = cheerio.load(html);
    const elements = $('[data-urn*="urn:li:activity"]');
    const posts: LinkedInPost[] = [];

    elements.each((_, element) => {
      const urn = $(element).attr('data-urn') ?? '';
      const content = $(element).find('[dir="ltr"]').text().trim();
      if (!urn || !content) return;

      const likes = this.extractNumber($(element), ['[aria-label*="reaction"]', '.social-counts-reactions span']);
      const comments = this.extractNumber($(element), ['[aria-label*="comment"]', '.social-counts-comments span']);
      const shares = this.extractNumber($(element), ['[aria-label*="share"]', '.social-counts-shares span']);

      const publishedAt = $(element).find('time').attr('datetime') ?? new Date().toISOString();
      const url = `https://www.linkedin.com/feed/update/${urn}`;

      posts.push({
        id: urn,
        content,
        publishedAt,
        engagement: { likes, comments, shares },
        url,
        author: username
      });
    });

    return posts;
  }

  private extractNumber(element: cheerio.Cheerio, selectors: string[]): number {
    for (const selector of selectors) {
      const text = element.find(selector).first().text().trim();
      if (!text) continue;
      const digits = text.match(/\d+(\.\d+)?/);
      if (digits) {
        const value = parseFloat(digits[0]);
        if (text.includes('K')) return Math.round(value * 1000);
        if (text.includes('M')) return Math.round(value * 1_000_000);
        return Math.round(value);
      }
    }
    return 0;
  }

  async exportCookies(): Promise<Protocol.Network.Cookie[]> {
    if (!this.page) return [];
    return this.page.cookies('https://www.linkedin.com/');
  }

  async close(): Promise<void> {
    if (this.browser) {
      await this.browser.close();
      this.browser = null;
      this.page = null;
    }
  }
}
```

```typescript
// services/worker/src/run-processor.ts
import { Job, Queue } from 'bullmq';
import { Protocol } from 'puppeteer';
import { config } from './config.js';
import { logger } from './logger.js';
import { getSupabase } from './supabase.js';
import { ensureLoginSession, captureCookiesFromSession, encryptCookies, decryptCookies } from './browser-login.js';
import { storeArtifact } from './storage.js';
import { RemoteScraper } from './remote-scraper.js';
import { buildKnowledgeBundle } from './knowledge-bundler.js';
import { InsightAnalyzer } from '../../src/analyzer.js';
import { InsightAnalysis } from '../../src/types.js';
import { ThreeFileFormatter } from '../../src/three-file-formatter.js';

export interface RunJobData {
  runId: string;
  userId: string;
}

interface RunRecord {
  id: string;
  user_id: string;
  status: string;
  config: {
    profileUrls: string[];
    topics: string[];
    outputFormat: 'ai-ready' | 'briefing';
    postLimit: number;
  };
}

interface SessionRecord {
  id: string;
  cookies: Protocol.Network.Cookie[];
  expiresAt: string | null;
}

interface LoginEventPayload {
  sessionId: string;
  wsEndpoint: string;
  connectUrl: string;
  expiresAt?: string;
}

const requeueQueue = new Queue<RunJobData>('insight-runs', {
  connection: { url: config.queueRedisUrl },
  defaultJobOptions: { removeOnComplete: true, removeOnFail: 1000 }
});

async function fetchRun(runId: string): Promise<RunRecord | null> {
  const supabase = getSupabase();
  const { data, error } = await supabase
    .from('runs')
    .select('id,user_id,status,config')
    .eq('id', runId)
    .single();
  if (error && error.code !== 'PGRST116') throw error;
  return data as RunRecord | null;
}

async function getActiveSession(userId: string): Promise<SessionRecord | null> {
  const supabase = getSupabase();
  const { data, error } = await supabase
    .from('linked_sessions')
    .select('id, encrypted_payload, expires_at')
    .eq('user_id', userId)
    .eq('is_active', true)
    .order('created_at', { ascending: false })
    .limit(1)
    .single();
  if (error && error.code !== 'PGRST116') throw error;
  if (!data) return null;
  return {
    id: data.id as string,
    cookies: decryptCookies(data.encrypted_payload as string),
    expiresAt: data.expires_at as string | null
  };
}

async function getLatestLoginEvent(runId: string): Promise<LoginEventPayload | null> {
  const supabase = getSupabase();
  const { data, error } = await supabase
    .from('run_events')
    .select('payload')
    .eq('run_id', runId)
    .eq('event_type', 'needs_login')
    .order('created_at', { ascending: false })
    .limit(1)
    .single();
  if (error && error.code !== 'PGRST116') throw error;
  return data ? data.payload as LoginEventPayload : null;
}

async function saveSession(userId: string, cookies: Protocol.Network.Cookie[], expiresAt?: string) {
  const supabase = getSupabase();
  const { payload } = encryptCookies(cookies);

  await supabase.from('linked_sessions')
    .update({ is_active: false })
    .eq('user_id', userId);

  const { error } = await supabase.from('linked_sessions').insert({
    user_id: userId,
    provider: 'browserless',
    encrypted_payload: payload,
    encryption_algorithm: 'aes-256-gcm',
    expires_at: expiresAt ?? null,
    is_active: true
  });
  if (error) throw error;
}

async function updateRunStatus(runId: string, status: string, payload: Record<string, unknown> = {}) {
  const supabase = getSupabase();
  const updates: Record<string, unknown> = { status };
  const now = new Date().toISOString();
  if (status === 'running') updates.started_at = now;
  if (status === 'completed') updates.completed_at = now;
  if (status === 'failed') updates.failure_reason = payload.reason ?? 'unknown';

  const { error } = await supabase.from('runs').update(updates).eq('id', runId);
  if (error) throw error;

  await supabase.from('run_events').insert({
    run_id: runId,
    event_type: `status_${status}`,
    payload
  });
}

async function incrementTokenUsage(userId: string, tokens: number) {
  const supabase = getSupabase();
  const periodStart = new Date().toISOString().slice(0, 7) + '-01';

  const { data, error } = await supabase
    .from('usage_counters')
    .select('runs_used, tokens_used')
    .eq('user_id', userId)
    .eq('period_start', periodStart)
    .single();
  if (error && error.code !== 'PGRST116') throw error;

  const runsUsed = data?.runs_used ?? 0;
  const tokensUsed = data?.tokens_used ?? 0;

  const { error: upsertError } = await supabase.from('usage_counters').upsert({
    user_id: userId,
    period_start: periodStart,
    runs_used,
    tokens_used: tokensUsed + tokens
  }, { onConflict: 'user_id,period_start' });
  if (upsertError) throw upsertError;
}

export async function processRunJob(data: RunJobData, job: Job<RunJobData>) {
  const run = await fetchRun(data.runId);
  if (!run) {
    logger.warn({ runId: data.runId }, 'Run not found, dropping job');
    return;
  }

  let session = await getActiveSession(run.user_id);
  if (!session) {
    const pending = await getLatestLoginEvent(run.id);
    if (!pending) {
      await ensureLoginSession(run.id, run.user_id);
      await requeueQueue.add('run', data, { delay: config.requeueDelayMs });
      return;
    }
    try {
      const cookies = await captureCookiesFromSession({
        sessionId: pending.sessionId,
        connectUrl: pending.connectUrl,
        wsEndpoint: pending.wsEndpoint,
        expiresAt: pending.expiresAt
      });
      if (!cookies.length) {
        await requeueQueue.add('run', data, { delay: config.requeueDelayMs });
        return;
      }
      await saveSession(run.user_id, cookies, pending.expiresAt);
      session = { id: 'captured', cookies, expiresAt: pending.expiresAt ?? null };
    } catch (error) {
      logger.warn({ runId: run.id, error }, 'Login session not ready, requeueing');
      await requeueQueue.add('run', data, { delay: config.requeueDelayMs });
      return;
    }
  }

  const scraper = new RemoteScraper({
    wsEndpoint: config.browserlessWsUrl,
    token: config.browserlessToken,
    cookies: session.cookies
  });

  try {
    await scraper.init();
    await updateRunStatus(run.id, 'running');

    const scrapeResult = await scraper.scrapeProfiles(run.config.profileUrls, run.config.postLimit, run.config.topics);
    const analyzer = new InsightAnalyzer();
    const aggregateAnalysis = await analyzer.analyzeInsights(scrapeResult.allPosts, {
      linkedinUsername: '',
      postLimit: run.config.postLimit,
      focusTopics: run.config.topics,
      outputFormat: run.config.outputFormat === 'ai-ready' ? 'instructions' : 'markdown'
    });

    const perExpertAnalysis: Record<string, InsightAnalysis> = {};
    // Re-run analyzer per expert to maintain expert-specific fidelity; optimization deferred.
    for (const [username, posts] of Object.entries(scrapeResult.byExpert)) {
      perExpertAnalysis[username] = await analyzer.analyzeInsights(posts, {
        linkedinUsername: username,
        postLimit: run.config.postLimit,
        focusTopics: run.config.topics,
        outputFormat: 'instructions'
      });
    }

    const formatter = new ThreeFileFormatter();
    const bundle = buildKnowledgeBundle(aggregateAnalysis, perExpertAnalysis, formatter, run.config.topics, run.config.postLimit);

    await storeArtifact(run.id, 'instructions', bundle.instructionsMarkdown);
    for (const expertFile of bundle.expertMarkdownFiles) {
      await storeArtifact(run.id, `expert-${expertFile.username}`, expertFile.content);
    }

    const refreshedCookies = await scraper.exportCookies();
    if (refreshedCookies.length) {
      await saveSession(run.user_id, refreshedCookies, session.expiresAt ?? null);
    }

    const tokensUsed = aggregateAnalysis.tokenUsage ?? 0;
    await incrementTokenUsage(run.user_id, tokensUsed);

    const supabase = getSupabase();
    await supabase.from('runs').update({
      status: 'completed',
      completed_at: new Date().toISOString(),
      token_estimate: tokensUsed,
      cost_estimate: Number(((tokensUsed / 1_000_000) * config.costPerMillionTokens).toFixed(4))
    }).eq('id', run.id);

    await supabase.from('run_events').insert({
      run_id: run.id,
      event_type: 'completed',
      payload: {
        tokensUsed,
        expertsAnalyzed: Object.keys(scrapeResult.byExpert).length,
        postsAnalyzed: scrapeResult.allPosts.length
      }
    });
  } catch (error: any) {
    logger.error({ runId: run.id, error }, 'Run processing failed');
    await updateRunStatus(run.id, 'failed', { reason: error.message ?? 'unknown_error' });
    throw error;
  } finally {
    await scraper.close();
  }
}
```

```typescript
// services/worker/src/index.ts
import { startWorker } from './queue.js';
import { logger } from './logger.js';

startWorker();
logger.info('Worker started');
```

```typescript
// services/worker/tests/run-processor.test.ts
import { describe, it, expect, vi, beforeEach } from 'vitest';
import * as processor from '../src/run-processor.js';
import * as supabaseMod from '../src/supabase.js';
import * as loginMod from '../src/browser-login.js';
import { Queue } from 'bullmq';

vi.mock('../src/supabase.js');
vi.mock('../src/browser-login.js');
vi.mock('bullmq', () => {
  return {
    Queue: vi.fn().mockImplementation(() => ({
      add: vi.fn()
    }))
  };
});
vi.mock('../src/remote-scraper.js', () => ({
  RemoteScraper: vi.fn().mockImplementation(() => ({
    init: vi.fn(),
    scrapeProfiles: vi.fn().mockResolvedValue({ allPosts: [], byExpert: {} }),
    exportCookies: vi.fn().mockResolvedValue([]),
    close: vi.fn()
  }))
}));
vi.mock('../../src/analyzer.js', () => ({
  InsightAnalyzer: vi.fn().mockImplementation(() => ({
    analyzeInsights: vi.fn().mockResolvedValue({ tokenUsage: 0, insights: [], templates: [] })
  }))
}));
vi.mock('../../src/three-file-formatter.js', () => ({
  ThreeFileFormatter: vi.fn().mockImplementation(() => ({
    formatThreeFiles: vi.fn().mockReturnValue({
      knowledgeBase: '# Knowledge',
      coreRules: '',
      projectInstructions: ''
    })
  }))
}));

describe('processRunJob', () => {
  const mockClient = { from: vi.fn() };

  beforeEach(() => {
    vi.resetAllMocks();
    (supabaseMod.getSupabase as unknown as vi.Mock).mockReturnValue(mockClient);
    (Queue as unknown as vi.Mock).mockReturnValue({ add: vi.fn() });
  });

  it('requests login when no active session', async () => {
    mockClient.from.mockReturnValue({
      select: vi.fn().mockReturnValue({
        eq: vi.fn().mockReturnValue({
          eq: vi.fn().mockReturnValue({
            single: vi.fn().mockResolvedValue({ data: { id: 'run', user_id: 'user', status: 'queued', config: { profileUrls: ['https://www.linkedin.com/in/test/'], topics: [], outputFormat: 'ai-ready', postLimit: 50 } }, error: null })
          })
        })
      })
    });
    (loginMod.ensureLoginSession as vi.Mock).mockResolvedValue(undefined);
    (loginMod.decryptCookies as vi.Mock).mockReturnValue([]);

    await processor.processRunJob({ runId: 'run', userId: 'user' }, { data: { runId: 'run', userId: 'user' } } as any);
    expect(loginMod.ensureLoginSession).toHaveBeenCalled();
  });
});
```

- **Tests**: `npm test` in `/services/worker` (Vitest).  
- **Telemetry**: Worker uses pino logger; run events recorded.  
- **Rollback**: remove `/services/worker`.

---

### T-005 – Telemetry & Logging Doc
- **Summary**: Document logging conventions, metrics capture, and future instrumentation.  
- **Rationale**: Aligns agents on structured logging fields and telemetry expectations.  
- **Outputs**: `docs/telemetry-and-logging.md`.

```markdown
# Telemetry & Logging Guidelines

## Logging
- Use `pino` loggers (`services/api/src/logger.ts`, `services/worker/src/logger.ts`).  
- Emit JSON with contextual fields: `scope`, `runId`, `userId`, `event`, `durationMs`, `error`.  
- Level guidance: `info` for state transitions, `warn` for recoverable issues (e.g., waiting for login), `error` for failures leading to retries or aborts.  
- Worker also records run events in Supabase `run_events` for UI visibility.

## Metrics
- Store per-run telemetry on `runs.token_estimate` and `runs.cost_estimate`.  
- Maintain monthly aggregates in `usage_counters` (`runs_used`, `tokens_used`).  
- Capture login prompts and completions by inserting run events (`needs_login`, `status_running`, `completed`, `failed`).

## Tracing
- Not in MVP scope.  
- Reserve `X-Request-Id` header for API so logs can be correlated when tracing is introduced later.

## Alerting
- Configure hosting provider alerts for worker failures (consecutive `failed` events) and queue backlog growth.  
- Monitor Browserless session usage vs. plan limits once vendor selected.
```

- **Rollback**: delete the doc if superseded.

---

### T-006 – Frontend Integration Contract
- **Summary**: Document API endpoints, headers, polling cadence, download mechanics for Lovable UI.  
- **Outputs**: `docs/frontend-contract.md`.

```markdown
# Frontend Integration Contract

## Auth
- Front end obtains Supabase access token (JWT) via Lovable/Supabase integration.  
- Include `Authorization: Bearer <supabase-token>` on every API call.  
- Backend verifies token and upserts Supabase user on first request.

## Endpoints
| Method | Path | Description |
| --- | --- | --- |
| GET | `/health` | API heartbeat. |
| GET | `/usage` | Returns `{ runsUsed, runsRemaining, tokensUsed, tokenCostEstimate }`. |
| GET | `/runs` | Latest runs (limit 100). |
| POST | `/runs` | Create run. Body `{ profileUrls, topics, outputFormat, nickname?, postLimit? }`. |
| GET | `/runs/{runId}` | Run detail including `needs_login_url` and telemetry. |

## Run Lifecycle
- States: `queued → needs_login → running → completed/failed`.  
- Poll `/runs/{id}` every 5s until terminal state.  
- On `needs_login_url`, show modal and open connect URL in new window/iframe; backend requeues job automatically.

## Downloads
- After completion, UI fetches artifact metadata (follow-up endpoint after MVP) and uses Supabase signed URLs.  
- Interim MVP: provide direct links to stored Markdown via Supabase storage helper.

## Error Handling
- `401` → redirect to Lovable login.  
- `429` → show “Run limit reached” message and CTA to upgrade (future).  
- Failed runs expose `failure_reason`; prompt user to retry.

## Request Headers Example
```
POST /runs
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
content-type: application/json
```
```

- **Rollback**: delete doc if API contract changes.

---

### T-007 – Cutover Checklist & QA
- **Summary**: Document launch checklist, manual QA steps, rollback procedure.  
- **Outputs**: `docs/cutover-checklist.md`.

```markdown
# MVP Cutover Checklist

## Pre-Cutover
- [ ] Apply Supabase migration to staging and production.  
- [ ] Provision Redis queue (Upstash) and store credentials.  
- [ ] Configure Browserless project (HTTP + WS URLs + API token).  
- [ ] Create Supabase storage bucket `runs` (private).  
- [ ] Connect Lovable UI to API base URL and validate flows.  
- [ ] Seed sample run artifacts for preview page.

## Launch Steps
1. Deploy `/services/api` with environment variables.  
2. Deploy `/services/worker` on persistent host (Fly/Render) with same env vars and queue access.  
3. Rotate `SESSION_ENCRYPTION_KEY` and `BROWSERLESS_TOKEN` via secret manager.  
4. Invite pilot users, monitor first runs.

## Manual QA
- [ ] Create account, view sample output.  
- [ ] Submit run with new profile → observe login prompt → log in → ensure run completes.  
- [ ] Download consolidated instructions and expert files; confirm Markdown content.  
- [ ] Verify usage counter drops from 5 to 4.  
- [ ] Trigger failure (invalid profile) and confirm UI error state + logged failure reason.

## Rollback
- API: redeploy previous build or scale to zero.  
- Worker: stop service; mark in-flight runs as failed with message “Run paused”.  
- Data: optionally remove runs created during rollback window.

## Post-Launch Monitoring
- Track worker failure rate (alert if >10% in 1 hour).  
- Watch Browserless session usage vs. plan.  
- Review weekly telemetry (runs, tokens) to inform pricing.
```

- **Rollback**: delete doc if replaced.

---

## Parallelization Plan

Streams:
- Stream A: [T-001, T-003, T-006] - touches: apps/web/**, packages/ui/**
- Stream B: [T-002, T-004] - touches: apps/api/**, packages/core/**
- Stream C: [T-005] - touches: migrations/** (schema + seeds)  - serialize with A/B (no parallel DB changes)

Rules:
- No stream may edit paths owned by another stream.
- Any task that changes shared types/config (e.g., packages/core/types/** or package.json) must run serialized and first in its stream.
- All streams commit to branch feature/<short-name> with message format: "[<PlanID>] T-00X - <summary>"
- Before every push: git pull --rebase; on conflict: stop and escalate to Coordinator.

## Testing Strategy
- **Unit**: Vitest suites for API run-limit logic, Supabase JWT middleware (happy/invalid paths), worker requeue behavior, and knowledge bundler per-expert output validation.  
- **Integration**: Post-MVP, add tests hitting Supabase test instance with mocked Browserless (not in initial scope).  
- **E2E**: Manual QA via Lovable UI flow outlined in T-007.  
- **Fixtures**: Provide sample run configuration + mocked LinkedIn posts for local testing (to be added when implementing automated tests).

## Telemetry and Logging
- API and worker emit structured logs via pino (JSON).  
- Include correlation IDs (use `X-Request-Id` header if present).  
- Key log fields: `runId`, `userId`, `event`, `durationMs`, `error`.  
- Run events inserted into Supabase for UI consumption.  
- Metrics captured via `runs` and `usage_counters` tables; future export to monitoring system.
- Worker logs should periodically record BullMQ queue depth (waiting count) to detect backlog early.

## Migration and Rollback
- Apply `supabase/migrations/0001_mvp_schema.sql` using Supabase CLI; rollback via paired script.  
- No data backfill required; schema idempotent.  
- Worker/API deploys can be rolled back independently; queue jobs will retry with previous binaries once redeployed.

## Backwards Compatibility and API Change Notes
- New API service introduces `/runs` and `/usage` endpoints; no existing clients rely on them yet.  
- Lovable UI is the only consumer; ensure feature flag or staging environment while iterating.  
- Cookie encryption format (`aes-256-gcm` base64 payload) defined here; future migrations must preserve compatibility.

## Security and Privacy Considerations
- LinkedIn session cookies stored encrypted (AES-256-GCM) with server-side secret.  
- No LinkedIn credentials captured; users log in directly via Browserless UI.  
- Ensure Browserless provider complies with data handling expectations; rotate tokens periodically.  
- `.env` secrets (Anthropic key, Browserless token, Supabase keys) managed via hosting secret stores.

## Performance Considerations and Budgets
- Run timeout budget: 20 minutes (includes scraping + analysis).  
- Remote scraper sequentially processes up to 10 experts; monitoring will inform tuning of post limits.  
- Queue requeue delay defaults to 5s; adjust based on Browserless login latency.  
- Worker should be deployed on instance with sufficient CPU/RAM to run Puppeteer connection (recommend 1 vCPU / 1GB).

## Risks, Mitigations, Decision Log
- **Per-expert bundle regression**: build per-expert analyses and add unit tests ensuring distinct expert outputs.  
- **Browserless provider mismatch**: request monthly login volume + pricing before implementation; abstract provider details in `browserless-client.ts`.  
- **LinkedIn anti-bot measures**: limit experts per run, use human login, throttle scrolling/pagination.  
- **Supabase schema drift**: migrations versioned; require review before apply.  
- **Queue backlog**: add monitoring on BullMQ + alert when waiting jobs exceed threshold.

## Acceptance Criteria and Manual QA Steps
- User can run end-to-end flow: create run, log in via streamed browser, obtain consolidated instructions + per-expert files.  
- Usage dashboard shows decremented runs and token estimate.  
- Failed login appropriately requeues and exposes retry instructions.  
- Docs (`project-requirements`, `telemetry`, `frontend-contract`, `cutover`) reviewed and accepted.

## Cutover Plan and Owner Checklist
- Owners: codex (backend/worker), product partner (QA & launch).  
- Pre-launch checklist from T-007 tracked in shared doc.  
- Confirm Browserless session budget and Supabase/Redis credentials before go-live.  
- After pilot launch, schedule follow-up review to assess telemetry and prioritize Priority-Two features.

## Change Log
- [2025-02-14 18:45 UTC] codex – aligned worker formatter workflow with existing utilities, added Supabase JWT auth coverage in tests, documented infra prerequisites, and noted signed URL decision.
- [2025-02-14 19:10 UTC] codex – ensured per-expert analyses drive knowledge bundles, clarified unit testing for bundler, and updated risk mitigation.
- [2025-02-14 19:25 UTC] codex – corrected aggregate token usage reference in worker pipeline.
- [2025-02-14 19:40 UTC] codex – documented per-expert re-analysis rationale and added queue depth logging guidance.
